[
  {
    "path": "posts/2025-01-05-visualizing-national-parks/",
    "title": "Mapping Visitor Trends in U.S. National Parks",
    "description": "Ever wondered how our visits to national parks fulfill deep-seated desires for adventure, tranquility, and a return to nature? Let’s explore this question with my visualizations for the 2025 Data Viz Championship.",
    "author": [
      {
        "name": "Guada Gonzalez",
        "url": "https://guadagonzalez.com/"
      }
    ],
    "date": "2025-01-05",
    "categories": [],
    "contents": "\r\nI’ve always been intrigued by how people use their spare time. The contest from the Big Ten Academic Alliance greatly helped me explore this topic using R. I took this opportunity to learn more about time use and discover what I had been missing in terms of visualization techniques.\r\nThe plots I created are organized by region, so let’s review which regions belong where on the US Map from the website of Park Chasers:\r\n\r\nNow that we can identify regions on the map, let’s start with a pie chart. I know the data science community might disapprove—pie charts (1), but I wanted to experiment with inserting an image as a background and overlaying a plot on it. The compass seems perfect for this. The first plot shows that the most visited parks in 2023 were in the Intermountain (34.8%) and Pacific West (23.32%) regions, which is logical since they include Yellowstone, Grand Canyon, Yosemite, and Olympic National Park. Conversely, the Alaska region, with only 1.5%, features fascinating wilderness areas but is remote, less accessible, and has harsher climates than other continental parks. Let’s look at the plot:\r\n\r\nHere’s the code for replication:\r\n\r\n\r\nShow code\r\n\r\nrm(list = ls())\r\n\r\n# Load Libraries ------------------------------------------------------------\r\n\r\npacman::p_load(\r\n  ggplot2,         # Easily Install and Load the 'Tidyverse'\r\n  grid,            # Improved Text Rendering Support for 'ggplot2'\r\n  ggpath,          # Using Fonts More Easily in R Graphs\r\n  jpeg,           # Simple Tools for Examining and Cleaning Dirty Data\r\n  magick,             # Compact and Flexible Summaries of Data\r\n  ggimage,            # Scale Functions for Visualization\r\n  gganimate,         # Make Dealing with Dates a Little Easier\r\n  ggtext,              # Interpreted String Literals\r\n  stringr,               # Pixel Filters for 'ggplot2' and 'grid' # Pixel Filters for 'ggplot2' and 'grid' \r\n  glue,\r\n  htmlwidgets,\r\n  webshot\r\n  )  \r\n\r\n# Configuration Plot ------------------------------------------------------\r\n\r\nfont_add_google(\"Oswald\", \"title\")\r\nfont_add_google(\"Merriweather Sans\", \"subtitle\")\r\nfont_add_google(\"Merriweather Sans\", \"text\")\r\nfont_add_google(\"Noto Sans\", \"caption\")\r\nshowtext_auto(enable = TRUE)\r\n\r\nimg = \"C:/Users/User/Documents/GitHub/2025_Data_Viz_Championship/compass2.jpg\"\r\n\r\nfa_path <- systemfonts::font_info(family = \"Font Awesome 6 Brands Regular\")[[\"path\"]]\r\nfont_add(family = \"fa-brands\", regular = fa_path)\r\n\r\nsubtitle_text <- str_glue(\"Although visits started as early as 1904, the NPS has been tracking them monthly since 1979 \\n in 63 parks across the USA. In 2023 alone, the different parks received more than 9.2 million visits, \\n distributed as shown in the plot.\")\r\n\r\ncaption_text  <- str_glue(\"{tt} <span style='font-family:fa6-brands'>&#xf08c;<\/span> guadag12 &bull; {tw} @guadag12 ; {gi} @guadag12\")\r\n\r\nregion_colors <-  c(\"Alaska\" = \"#B0C4DE\", \"Intermountain\" = \"#EDC9AF\", \r\n                    \"Midwest\" = \"#6B8E23\", \"Northeast\" = \"#708090\", \r\n                    \"Pacific West\" = \"#DEB887\", \"Southeast\" = \"#5F9EA0\")\r\n\r\n\r\ntheme_update(\r\n  plot.title.position   = \"plot\",\r\n  plot.caption.position = \"plot\",\r\n  legend.position       = 'plot',\r\n  plot.background       = element_rect(fill = bkg_col, color = bkg_col),\r\n  panel.background      = element_rect(fill = bkg_col, color = bkg_col),\r\n  plot.margin           = margin(t = 10, r = 20, b = 10, l = 20),\r\n  \r\n  axis.title.x          = element_text(margin = margin(10, 0, 0, 0), size = rel(1.1), \r\n                                       color = text_col, family = \"text\", face = \"bold\", hjust = 0.5),\r\n  axis.title.y          = element_text(margin = margin(10, 0, 0, 0), size = rel(1.1), \r\n                                       color = text_col, family = \"text\", face = \"bold\", hjust = 0.5,\r\n                                       angle = 90),  \r\n  axis.text.y           =  element_text(color = text_col, family = \"text\", size = rel(0.9)),\r\n  axis.text.x           = element_text(color = text_col, family = \"text\", size = rel(0.9)),\r\n  axis.ticks.x          = element_line(color = text_col),  # Show x-axis ticks\r\n  panel.grid            = element_blank(),\r\n  strip.text            = element_blank() # Remove strip text \r\n)\r\n\r\nregion_colors <-  c(\"Alaska\" = \"#B0C4DE\", \"Intermountain\" = \"#EDC9AF\", \r\n                    \"Midwest\" = \"#6B8E23\", \"Northeast\" = \"#708090\", \r\n                    \"Pacific West\" = \"#DEB887\", \"Southeast\" = \"#5F9EA0\")\r\n\r\n# Plot --------------------------------------------------------------------\r\n\r\np <-NPData %>%\r\n  group_by(Region) %>%\r\n  summarise(total_visit = sum(RecreationVisits)) %>%\r\n  mutate(prop = total_visit / sum(total_visit) * 100) %>%\r\n  mutate(ypos = cumsum(prop) - 0.7 * prop) %>%\r\n  mutate(ypos_mod = case_when(\r\n    round(ypos, 1) == 0.450 ~ -10,\r\n    round(ypos, 1) == 87.1 ~ 12.9 ,\r\n    round(ypos, 1) == 65.3 ~ 28.7 ,\r\n    round(ypos, 1) == 53.0 ~ 45.0,\r\n    round(ypos, 1) == 40.7 ~ 58.3,\r\n    round(ypos, 1) == 11.9  ~ 80.1,\r\n    TRUE ~ ypos\r\n  )) %>%\r\n  ggplot( aes(x=\"\", y=prop, fill=Region)) +\r\n  with_inner_glow(\r\n  geom_bar(stat=\"identity\", width=0.01, alpha =.95) ,\r\n  color = \"gray10\", sigma = 15\r\n  ) +\r\n  coord_polar(\"y\", start=0) + \r\n  theme_void() +\r\n  labs(\r\n    title = \"The Story of Recreation Visits\",\r\n    subtitle = subtitle_text,\r\n    caption = \"Source: National Park Service | Graphics by @guadag12\"\r\n  ) +\r\n  scale_fill_manual(values = region_colors) +  \r\n  force_panelsizes(rows = unit(2, \"in\"), cols = unit(3.5, \"in\")) +\r\n  theme(\r\n    plot.background = element_rect(fill = \"transparent\", colour = NA),\r\n    panel.background = element_rect(fill = \"transparent\", colour = NA), \r\n    plot.margin = unit(c(-1.3, 1.7, 0, 0), \"cm\"),\r\n    panel.spacing = unit(c(0.5, 0, 0, 0), \"cm\"),\r\n    legend.position = \"none\",\r\n    plot.title = element_text(family = \"title\", size = 20, face = \"bold\", vjust = 13,hjust = -10),\r\n    plot.subtitle = element_text(family = \"subtitle\", size = 9, vjust = 20, hjust = 0.5),\r\n    plot.caption = element_text(family = \"caption\", size = 10, vjust = -15),\r\n  )\r\nggbackground(p + geom_text(\r\n  aes(y = ypos_mod,  label = paste(Region, \"\\n (\", sprintf(\"%.1f%%\", prop), \")\"), group = Region, angle = 10), \r\n  size = 2.5,\r\n  check_overlap = F, \r\n  color = \"#2F4F4F\", family = \"text\") , img)\r\n\r\n#webshot::install_phantomjs(force = T)\r\nwebshot(\"http://localhost:31220/session/preview.html\", file = \"viewer_screenshot.png\")\r\n\r\n\r\nHowever, knowing which National Parks were most visited in 2023 is not enough to understand how people spend their time. What if we use a line plot to observe the evolution of recreational visits to National Parks, differentiated by region?\r\nInspired by this visualization, the second plot includes this information but with a twist: the names of the parks appear in gigantic letters on the plot.\r\n\r\nThis second plot displays the trends in recreational visits (in millions) to National Parks across various U.S. regions from 1980 to 2023. Each line represents a region and shows how visitation has changed over 40 years. Notably, all regions have seen an increase in the number of visitors over the decades, particularly around 2010 and just before the pandemic. The pandemic led to a significant reduction in visits, followed by a rebound, possibly as lockdowns increased people’s desire to spend more time in nature.\r\nHere’s the code for replication:\r\n\r\n\r\nShow code\r\n\r\noptions(scipen = 999)\r\nrm(list =ls())\r\n\r\npacman::p_load(\r\n  tidyverse,         # Easily Install and Load the 'Tidyverse'\r\n  ggtext,            # Improved Text Rendering Support for 'ggplot2'\r\n  showtext,          # Using Fonts More Easily in R Graphs\r\n  janitor,           # Simple Tools for Examining and Cleaning Dirty Data\r\n  skimr,             # Compact and Flexible Summaries of Data\r\n  scales,            # Scale Functions for Visualization\r\n  lubridate,         # Make Dealing with Dates a Little Easier\r\n  glue,              # Interpreted String Literals\r\n  ggfx               # Pixel Filters for 'ggplot2' and 'grid' # Pixel Filters for 'ggplot2' and 'grid' \r\n)  \r\ncamcorder::gg_record(\r\n  dir    = here::here(\"temp_plots\"),\r\n  device = \"png\",\r\n  width  =  7.5,\r\n  height =  5,\r\n  units  = \"in\",\r\n  dpi    = 320\r\n)\r\n\r\n### |- resolution ----\r\nshowtext_opts(dpi = 320, regular.wt = 300, bold.wt = 800)\r\n\r\n\r\n# Transformation ----------------------------------------------------------\r\n\r\nNPData<- read.csv(\"https://raw.githubusercontent.com/melaniewalsh/responsible-datasets-in-context/main/datasets/national-parks/US-National-Parks_RecreationVisits_1979-2023.csv\")\r\nNPData_groupby <- NPData %>%\r\n  group_by(Region, Year) %>%\r\n  summarise(total_visits = sum(RecreationVisits))\r\n\r\nunique(NPData_groupby$Region)\r\nlabels <- tibble(\r\n  label = c(\"Alaska\", \"Intermountain\", \"Midwest\", \"Northeast\", \"Pacific West\",\r\n            \"Southeast\"),\r\n  player = c(\"Alaska\", \"Intermountain\", \"Midwest\", \"Northeast\", \"Pacific West\",\r\n             \"Southeast\"),\r\n  x = c(2000, 2000, 2000, 2000, 2000, 2000), \r\n  y = c(0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001) \r\n)\r\n\r\n# Configuration Plot -----------------------------------------------------------\r\n\r\nloadfonts(device = \"win\")  # Usa device = \"win\" para Windows, \"quartz\" para Mac, y \"cairo\" para Linux\r\n\r\nbkg_col      <- colorspace::lighten('#ffffff', 0.05)    \r\ntitle_col    <- \"gray20\"           \r\nsubtitle_col <- \"gray20\"     \r\ncaption_col  <- \"gray30\"   \r\ntext_col     <- \"gray20\"    \r\ncol_palette  <- paletteer::paletteer_d(\"peRReo::don\")[c(1:6)]\r\n\r\n### |-  titles and caption ----\r\n# text\r\ntitle_text    <- str_glue(\"The evolution of Recreational Visits to National Parks\")\r\nsubtitle_text <- str_glue(\"How the number of visits to national parks has evolved over the last 45 years in the US\")\r\ncaption_text <-paste0(\"Source: National Park Service | Graphics by @guadag12\")\r\n\r\n### fonts ----\r\nfont_add_google(\"Oswald\", \"title\")\r\nfont_add_google(\"Merriweather Sans\", \"subtitle\")\r\nfont_add_google(\"Merriweather Sans\", \"text\")\r\nfont_add_google(\"Noto Sans\", \"caption\")\r\nshowtext_auto(enable = TRUE)\r\n\r\n### plot theme ----\r\ntheme_set(theme_minimal(base_size = 14, base_family = \"text\"))                \r\n\r\ntheme_update(\r\n  plot.title.position   = \"plot\",\r\n  plot.caption.position = \"plot\",\r\n  legend.position       = 'plot',\r\n  plot.background       = element_rect(fill = bkg_col, color = bkg_col),\r\n  panel.background      = element_rect(fill = bkg_col, color = bkg_col),\r\n  plot.margin           = margin(t = 10, r = 20, b = 10, l = 20),\r\n  \r\n   axis.title.x         = element_text(margin = margin(10, 0, 0, 0), size = rel(1.1), \r\n                                       color = text_col, family = \"text\", face = \"bold\", hjust = 0.5),\r\n  axis.title.y          = element_text(margin = margin(10, 0, 0, 0), size = rel(1.1), \r\n                                       color = text_col, family = \"text\", face = \"bold\", hjust = 0.5,\r\n                                       angle = 90),  \r\n  axis.text.y           = element_text(color = text_col, family = \"text\", size = rel(0.9)),\r\n  axis.text.x           = element_text(color = text_col, family = \"text\", size = rel(0.9)),\r\n  axis.ticks.x          = element_line(color = text_col),  # Show x-axis ticks\r\n  panel.grid            = element_blank(),\r\n  strip.text            = element_blank() # Remove strip text \r\n)\r\n\r\n\r\n\r\n\r\n# Plot --------------------------------------------------------------------\r\n\r\nregion_colors <-  c(\"Alaska\" = \"#B0C4DE\", \"Intermountain\" = \"#EDC9AF\", \r\n                   \"Midwest\" = \"#6B8E23\", \"Northeast\" = \"#708090\", \r\n                    \"Pacific West\" = \"#DEB887\", \"Southeast\" = \"#5F9EA0\")\r\nNPData_groupby <- NPData_groupby %>%\r\n  rename(player=Region)\r\nplot <- ggplot(NPData_groupby, aes(x = Year, y = total_visits)) +\r\n  # Reference text layer\r\n  as_reference(\r\n    geom_text(\r\n      data = labels, aes(x = x, y = y, label = label),\r\n      inherit.aes = FALSE,\r\n      family = \"title\", colour = \"gray10\", size = rel(10), hjust = 0.5, vjust = 0,\r\n        \r\n    ),\r\n    id = \"text\"\r\n  ) +\r\n  \r\n  # Blending the text with the density plot\r\n  with_inner_glow(\r\n    with_blend(\r\n      geom_area(aes(fill = player, color = player),\r\n                   alpha = 1, show.legend = FALSE#,\r\n                  # bw = 25,\r\n                  # kernel = \"epanechnikov\"\r\n      ),\r\n      bg_layer = \"text\", blend_type = \"xor\"\r\n    ),\r\n    color = \"gray10\", sigma = 15\r\n  ) +\r\n  \r\n  # Labs\r\n  labs(\r\n    x = \"Year\",\r\n    y = \"Total Visits\",\r\n    title = title_text,\r\n    subtitle = subtitle_text,\r\n    caption = caption_text\r\n  ) +\r\n  \r\n  # Scales\r\n  scale_x_continuous() +\r\n  scale_y_continuous( labels = label_number(suffix = \" M\", scale = 1e-6)) +\r\n  scale_fill_manual(values = col_palette) +\r\n  scale_color_manual(values = col_palette) +\r\n  coord_cartesian(clip = \"off\") +\r\n    scale_fill_manual(values = region_colors) +  \r\n\r\n  \r\n  # Facet\r\n  facet_wrap(~player, ncol = 2) +\r\n  \r\n  # Facet\r\n  theme(\r\n    plot.title = element_text(\r\n      size = rel(1.55),\r\n      family = \"title\",\r\n      face = \"bold\",\r\n      color = title_col,\r\n      lineheight = 1.1,\r\n      margin = margin(t = 5, b = 5)\r\n    ),\r\n    plot.subtitle = element_text(\r\n      size = rel(0.85),\r\n      family = \"subtitle\",\r\n      color = subtitle_col,\r\n      lineheight = 1.1,\r\n      margin = margin(t = 5, b = 5)\r\n    ),\r\n    plot.caption = element_markdown(\r\n      size = rel(0.50),\r\n      family = \"caption\",\r\n      color = caption_col,\r\n      lineheight = 1.1,\r\n      hjust = 0.5,\r\n      halign = 1,\r\n      margin = margin(t = 5, b = 5)\r\n    )\r\n  )\r\n\r\n\r\nIn conclusion, it is wonderful to see that more people are spending their time outside and this exploration is critical during 2010 and after the pandemic, as it created a new necessity of connecting more with nature.\r\nLast but not least, I want to share with you a a photo of me after a 4-hour, 14-km trek at the Otto Meiling Refuge in Nahuel Huapi National Park, Argentina:\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-01-05-visualizing-national-parks/plots/intro.png",
    "last_modified": "2025-01-06T10:32:18-03:00",
    "input_file": {},
    "preview_width": 880,
    "preview_height": 544
  },
  {
    "path": "posts/2024-03-03-introduction-to-selenium-with-twitter-data/",
    "title": "Introduction to Selenium with Twitter Data",
    "description": "Let's scrape Twitter data using Selenium",
    "author": [
      {
        "name": "Guada Gonzalez",
        "url": "https://guadagonzalez.com/"
      }
    ],
    "date": "2024-03-03",
    "categories": [],
    "contents": "\r\nScraping means ‘to gather, collect’ and it’s a very important tool that we social scientists have to acquire innovative and valuable information that not everyone has access to.\r\nIn general, we scrape web pages using two methods. One is based on HTML code, which is simpler, and the other is based on Javascript, which is a bit more complex. For scraping HTML, there are very good tutorials like this, this, and this, using packages like Beautifulsoup in Python and rvest in R. In this case, we will use Selenium in Python to obtain the number of followers and followings that legislators have in Argentina.\r\nIt’s not necessary to know Javascript to use Selenium, but it is necessary to be able to identify, in this case, classes within the source codes of the websites. That is, to understand a little bit of HTML and CSS.\r\nDisclaimer to expand the issue:\r\nIn HTML, elements have an ID and a class. An “ID” is ___ and a class is “__“. In this case, IDs are indicated with”#” in front and classes with a dot “.” in front.\r\nDownloading the number of followers and followings of legislators. For this, we will first try with a single account and then incorporate it into a loop.\r\nFor this, the following packages are necessary:\r\n\r\nimport random\r\nimport BeautifulSoup\r\nfrom time import sleep\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.common.action_chains import ActionChains\r\nimport re\r\nimport json\r\n\r\nThirdly, we call the driver. What is this?_\r\n\r\nWhen we run it automatically, we’ll see that an internet tab opens:\r\n[Image]\r\nThen we specify the URL we want to scrape together with the get() function that will send us to the web. In this case, for example, we want to scrape my Twitter account.\r\n\r\ndriver_ = webdriver.Chrome()\r\n\r\nurl = 'https://twitter.com/guadag12'\r\ndriver_.get(url)\r\nsleep(2)  # Adding sleep to ensure page loads. Consider using WebDriverWait for better practice.\r\n\r\nWebDriverWait(driver_, 10).until(\r\n    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".css-1rynq56.r-bcqeeo.r-qvutc0.r-37j5jr.r-a023e6.r-rjixqe.r-16dba41.r-1loqt21\"))\r\n)\r\n\r\nOnce we run it, we need to see how it opens the new tab.\r\n! Warning: It’s always good practice to give seconds of rest between visiting a website and scraping it, so we give the browser time to load and likewise, we don’t overload the website. Using the “sleep()” function implies being responsible with scraping use because ultimately, it’s a tool that forces the extraction of information in a context where we don’t necessarily ask the company for information. So let’s be responsible and respectful and put a sleep occasionally.\r\nThen we search on the web for the id/class of the element we want to scrape. In this case, as we want the number of followers and followings, we use this:\r\nWe use the “find_elements()” function and ask it to bring us an element determined by a CSS feature such as class, and we specify the class as a string:\r\n[Image]\r\nWhat it will automatically do is bring that data, and then we ask it to print it, and it will return something like this.\r\n\r\nh4_elements = driver_.find_elements(By.CSS_SELECTOR, \".css-1rynq56.r-bcqeeo.r-qvutc0.r-37j5jr.r-a023e6.r-rjixqe.r-16dba41.r-1loqt21\")\r\ntype_projects = [element.text.strip() for element in h4_elements]\r\nprint(type_projects)\r\n\r\nparsed_numbers = [parse_number(num) for num in type_projects]\r\n\r\nfollowing = parsed_numbers[0]\r\nfollowers = parsed_numbers[1]\r\n\r\nuser_data_new = {\r\n    \"User\": url,\r\n    \"Following\": following,\r\n    \"Followers\": followers\r\n}\r\n\r\nOnce we finish, we close the session:\r\n\r\ndriver_.quit()\r\n\r\nLOOP with Twitter accounts of Argentine legislators:\r\nWhat if we want to extract data from a large number of accounts? We need to\r\nBring the dataset with the accounts of Argentine legislators\r\nCreate an empty list to fill in later and\r\nPerform a for loop where in each iteration the information of each user is obtained\r\nBut before showing how the code would look like to do this, it’s important to know first that there are cases where legislators appear with “1.1k followers” or “1.3M followers” (millions). What we want to do is replace those “k” or “M” with the corresponding number of zeros. In this regard, this function is created to be used within the loop:\r\n\r\n\r\n\r\nNow, this is how the final loop would look like:\r\nAnd in this file, you can observe the final data:\r\nFor example, if we want to see which block has the highest number of followers on Twitter according to the number of block members, we see that:\r\n\r\n\r\n\r\nThis has been everything. Thank you for getting here and reading to the end,\r\nLeave comments for any questions,\r\nGuada\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-10-08T22:41:57-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-03-politicxs-en-twitter-again-online/",
    "title": "Analyzing political candidates with politicxsentwitteR!",
    "description": "Discover the revamped PoliticxsentwitteR website! Analyze political tweets, uncover engagement, words, emojis, and retweet dynamics in Argentina. Download data for deeper insights. Explore it! 🚀💻",
    "author": [
      {
        "name": "Guadalupe Gonzalez",
        "url": "https://guadagonzalez.com/"
      }
    ],
    "date": "2023-05-09",
    "categories": [
      "R",
      "STATS",
      "Network Analysis",
      "politicxsentwitteR"
    ],
    "contents": "\r\nFrom the Network Observatory together with Democracia en Red and the National Democratic Institute we are excited to announce the relaunch of our PoliticxsentwitteR website 🐦📊, now with updated data up to June 2022! 🎉\r\n\r\n📢 The app that I have the honor to develop analyzes tweets from politicians in Argentina  and provides insights into their engagement, the words and emojis that they use, and also who retweets who. It also provides the opportunity to free download the tweets so they can be analyzed and give users a more comprehensive view of political discourse on Twitter.\r\nWe hope that this update will provide valuable insights into political discourse in Argentina, and we encourage you to give it a try 🤩. You can access the app by visiting this link🔗.\r\n\r\n\r\n\r\n\r\nAnother important announcement is that politicxsintwitteR R package 📦 is also available to make the data as much accessible as possible. In this post I showed you what opportunities the package bring! Also, you can check this social network tutorial that I present thanks to R ladies Buenos Aires using politicxsintwitteR.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-03-politicxs-en-twitter-again-online/logo.png",
    "last_modified": "2023-05-10T13:06:50-03:00",
    "input_file": {},
    "preview_width": 1040,
    "preview_height": 1204
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to my new blog!",
    "description": "I hope you enjoy reading what I have to say!",
    "author": [
      {
        "name": "Guadalupe Gonzalez",
        "url": "https://github.com/guadag12"
      }
    ],
    "date": "2023-04-05",
    "categories": [],
    "contents": "\r\nWelcome to my blog! 👋 I’m excited to share our passion for quantitative methods, political science,\r\nand public opinion research with you 🔍. My goal is to provide a space where researchers, academics,\r\nand students can learn and discuss the developments in these fields.\r\nOne of the core aspects of my blog is the commitment to reproducibility ♻️. I believe that research should be\r\ntransparent and accessible to everyone, and one way to achieve this is by ensuring that our analyses can be easily replicated. That’s why I will be offering step-by-step tutorials on how to reproduce published studies using R .\r\nI will cover a broad range of topics, including statistical analysis, social network, public opinion analysis, text mining, deep learning, and much more 📊. Whether you’re an experienced researcher or just starting out, we hope that our blog will provide you with valuable insights and practical tips for your work.\r\nI encourage you to engage through comments and discussions 📥. My blog is a collaborative space where we can learn from each other and exchange ideas. Feel free to DM me about feedback, question or even new ideas suggestions for future blog posts!\r\nThank you for joining me on this journey 🙏. I am excited to share my expertise and learn from you!\r\nBest ❤️,\r\nGuada\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-05-12T07:14:44-03:00",
    "input_file": {}
  }
]
